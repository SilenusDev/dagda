# cauldron/muirdris/llama/pod.yml
# Configuration du service Llama pour Dagda-Lite
# Mod√®le de langage Llama

apiVersion: v1
kind: Pod
metadata:
  name: dagda-lite-llama-pod
spec:
  containers:
  - name: dagda-lite-llama
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
    - containerPort: ${LLAMA_CONTAINER_PORT}
      hostPort: ${LLAMA_PORT}
    volumeMounts:
    - name: llama-data
      mountPath: /models
    - name: llama-config
      mountPath: /config
  volumes:
  - name: llama-data
    hostPath:
      path: ${MUIRDRIS_DIR}/llama/data
      type: DirectoryOrCreate
  - name: llama-config
    hostPath:
      path: ${MUIRDRIS_DIR}/llama/config
      type: DirectoryOrCreate
  restartPolicy: Always